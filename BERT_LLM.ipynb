{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NatKu2_nlaKB",
        "outputId": "724b25e1-e123-44e8-beb6-50a38ff3e964"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DLANBp03pXBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "fFMWPA-upYYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/tweet_sentiment.csv')  # Replace with your dataset path\n",
        "\n",
        "# Check for NaN values\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Drop rows with NaN values\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "YAfewmFupYVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remap labels to 0, 1, 2\n",
        "df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n",
        "\n",
        "# Proceed with the rest of the code (splitting, tokenizing, etc.)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['cleaned_tweets'].values, df['sentiment'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "p3gK4pCNpYSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset class\n",
        "class FinancialTweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "x_fpUahepYQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "train_dataset = FinancialTweetDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = FinancialTweetDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "cBjpCZ7KpYNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)"
      ],
      "metadata": {
        "id": "iXcVKS1kpYLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = SentimentClassifier(n_classes=3)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "q7iq27R6pYJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"
      ],
      "metadata": {
        "id": "yEAZ8_GhpYHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def eval_model(model, data_loader, loss_fn, device):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n"
      ],
      "metadata": {
        "id": "5kWFv_0Cp3Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_loader,\n",
        "        loss_fn,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "MUzrJPxDp29o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'bert_sentiment_model.pth')\n"
      ],
      "metadata": {
        "id": "rT1KFAXKp27a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on validation set\n",
        "y_pred, y_true = [], []\n",
        "with torch.no_grad():\n",
        "    for d in val_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"label\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "# Remap predictions back to original labels\n",
        "y_pred = [0 if pred == 0 else 1 if pred == 1 else -1 for pred in y_pred]\n",
        "y_true = [0 if true == 0 else 1 if true == 1 else -1 for true in y_true]\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "udgWsAnCp25G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine code"
      ],
      "metadata": {
        "id": "aWKa2Arkqidk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/tweet_sentiment.csv')  # Replace with your dataset path\n",
        "\n",
        "# Check for NaN values\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Drop rows with NaN values\n",
        "df = df.dropna()\n",
        "\n",
        "# Remap labels to 0, 1, 2\n",
        "df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n",
        "\n",
        "# Proceed with the rest of the code (splitting, tokenizing, etc.)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['cleaned_tweets'].values, df['sentiment'].values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Continue with the DataLoader, model setup, and training as before\n",
        "\n",
        "\n",
        "# Create a custom dataset class\n",
        "class FinancialTweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = FinancialTweetDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = FinancialTweetDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Initialize the model\n",
        "model = SentimentClassifier(n_classes=3)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "# Evaluation function\n",
        "def eval_model(model, data_loader, loss_fn, device):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
        "\n",
        "# Training loop\n",
        "epochs = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_loader,\n",
        "        loss_fn,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'bert_sentiment_model.pth')\n",
        "\n",
        "# Evaluation on validation set\n",
        "y_pred, y_true = [], []\n",
        "with torch.no_grad():\n",
        "    for d in val_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"label\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "# Remap predictions back to original labels\n",
        "y_pred = [0 if pred == 0 else 1 if pred == 1 else -1 for pred in y_pred]\n",
        "y_true = [0 if true == 0 else 1 if true == 1 else -1 for true in y_true]\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFxkxTh6ltfX",
        "outputId": "df30c0be-62a5-4361-f544-08333dddf65e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned_tweets    6\n",
            "sentiment         0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "----------\n",
            "Train loss 0.2569786893700298 accuracy 0.9113729282982371\n",
            "Val   loss 0.09348433883468296 accuracy 0.978371724986812\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "Train loss 0.07995040027865795 accuracy 0.9773596518222183\n",
            "Val   loss 0.07414493424192678 accuracy 0.978371724986812\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "Train loss 0.04549281252977902 accuracy 0.9863278674110871\n",
            "Val   loss 0.053423930375654974 accuracy 0.9876912256022508\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "Train loss 0.029937692272809888 accuracy 0.9908559370466434\n",
            "Val   loss 0.047758635750040086 accuracy 0.9889221030420257\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.99      0.99      0.99      1700\n",
            "           0       0.95      0.97      0.96       502\n",
            "           1       0.99      0.99      0.99      3485\n",
            "\n",
            "    accuracy                           0.99      5687\n",
            "   macro avg       0.98      0.98      0.98      5687\n",
            "weighted avg       0.99      0.99      0.99      5687\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4N59dUsUnEmi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}